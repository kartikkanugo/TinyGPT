{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30ddd3b",
   "metadata": {},
   "source": [
    "# Coding Attention Mechanisms\n",
    "\n",
    "Now we will work on coding attention mechanisms.\n",
    "\n",
    "**Attention** is a mechanism that lets a model decide which parts of the input are most relevant when generating each part of the output.\n",
    "\n",
    "**Self-Attention** is a specific type of attention that allows each position in the input sequence to consider the relevancy of, or “attend to,” all other positions in the same sequence when computing the representation. Self-attention is a key component of contemporary LLMs based on the Transformer architecture, such as the GPT series.\n",
    "\n",
    "## The “Self” in Self-Attention\n",
    "\n",
    "In self-attention, the “self” refers to the mechanism’s ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image.\n",
    "\n",
    "This is in contrast to traditional attention mechanisms, where the focus is on the relationships between elements of two different sequences, such as in sequence-to-sequence models. For example, attention might be computed between an input sequence and an output sequence in translation models.\n",
    "\n",
    "## How Attention Works\n",
    "\n",
    "In a Transformer (the architecture behind LLMs), attention works using three main vectors for each token (word or subword):\n",
    "\n",
    "- **Query (Q)** – What am I looking for?\n",
    "\n",
    "- **Key (K)** – What do I contain?\n",
    "\n",
    "- **Value (V)** – What information do I carry?\n",
    "\n",
    "The model computes how much each word should “attend” to others by comparing the query of the current word with the keys of all other words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ac733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw attention scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "--------------------------------------------------\n",
      "Attention weights (normalized by sum): tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum of weights: tensor(1.0000)\n",
      "--------------------------------------------------\n",
      "Attention weights (naive softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum of weights: tensor(1.)\n",
      "--------------------------------------------------\n",
      "Attention weights (PyTorch softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum of weights: tensor(1.)\n",
      "--------------------------------------------------\n",
      "Context vector (output of attention): tensor([0.4419, 0.6515, 0.5683])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# Input sequence embeddings\n",
    "# Each row represents a token in the sequence (e.g., words) and their embedding vectors\n",
    "# -----------------------------\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # \"Your\"     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # \"journey\"  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # \"starts\"   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # \"with\"     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # \"one\"      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # \"step\"     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Select the query vector\n",
    "# Here, we are computing attention for the second word \"journey\" (x^2)\n",
    "# -----------------------------\n",
    "query = inputs[1]\n",
    "\n",
    "# -----------------------------\n",
    "# Compute raw attention scores\n",
    "# Dot product between the query and each input token\n",
    "# Higher score = more relevant token for this query\n",
    "# -----------------------------\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])  # preallocate tensor for scores\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(\"Raw attention scores:\", attn_scores_2)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Normalize attention scores (simple sum normalization)\n",
    "# Sum of weights = 1 (not ideal, usually softmax is better)\n",
    "# -----------------------------\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights (normalized by sum):\", attn_weights_2_tmp)\n",
    "print(\"Sum of weights:\", attn_weights_2_tmp.sum())\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Define naive softmax function\n",
    "# Converts raw scores into probabilities (weights) between 0 and 1\n",
    "# -----------------------------\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights (naive softmax):\", attn_weights_2_naive)\n",
    "print(\"Sum of weights:\", attn_weights_2_naive.sum())\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Use PyTorch's built-in softmax (numerically stable)\n",
    "# -----------------------------\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights (PyTorch softmax):\", attn_weights_2)\n",
    "print(\"Sum of weights:\", attn_weights_2.sum())\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute context vector\n",
    "# Weighted sum of all input embeddings based on attention weights\n",
    "# This is the output of the self-attention mechanism for the query\n",
    "# -----------------------------\n",
    "context_vec_2 = torch.zeros(query.shape)  # initialize context vector\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(\"Context vector (output of attention):\", context_vec_2)\n",
    "print(50 * \"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535a29e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw attention scores (manual):\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "--------------------------------------------------\n",
      "Raw attention scores (matrix multiplication):\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "--------------------------------------------------\n",
      "Attention weights (softmax):\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "--------------------------------------------------\n",
      "Context vectors (output of self-attention):\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Computing attention weights for all tokens\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# Input sequence embeddings\n",
    "# Each row represents a token in the sequence (e.g., words) and their embedding vectors\n",
    "# -----------------------------\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # \"Your\"     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # \"journey\"  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # \"starts\"   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # \"with\"     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # \"one\"      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # \"step\"     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute raw attention scores manually\n",
    "# attn_scores[i, j] = dot product between token i and token j\n",
    "# This measures how much token i should attend to token j\n",
    "# -----------------------------\n",
    "attn_scores = torch.empty(6, 6)  # initialize 6x6 matrix\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)  # dot product = similarity\n",
    "\n",
    "print(\"Raw attention scores (manual):\")\n",
    "print(attn_scores)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute attention scores using matrix multiplication (faster)\n",
    "# inputs @ inputs.T computes all dot products at once\n",
    "# -----------------------------\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(\"Raw attention scores (matrix multiplication):\")\n",
    "print(attn_scores)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Apply softmax along each row\n",
    "# Converts raw scores into attention weights (probabilities) for each token\n",
    "# Each row sums to 1\n",
    "# -----------------------------\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(\"Attention weights (softmax):\")\n",
    "print(attn_weights)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# Compute context vectors for all tokens\n",
    "# Multiply attention weights with input embeddings\n",
    "# This gives the new representation of each token after attending to all tokens\n",
    "# -----------------------------\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(\"Context vectors (output of self-attention):\")\n",
    "print(all_context_vecs)\n",
    "print(50 * \"-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d29864",
   "metadata": {},
   "source": [
    "### THE RATIONALE BEHIND SCALED-DOT PRODUCT ATTENTION\n",
    "\n",
    "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "\n",
    "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention.\n",
    "\n",
    "**Look at implementation of TinySelfAttentionQKV**\n",
    "\n",
    "Note that nn.Linear in SelfAttention_v2 uses a different weight initialization scheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1, which causes both mechanisms to produce different results. To check that both implementations, SelfAttention_v1 and SelfAttention_v2, are otherwise similar, we can transfer the weight matrices from a SelfAttention_v2 object to a SelfAttention_v1, such that both objects then produce the same results.\n",
    "\n",
    "Your task is to correctly assign the weights from an instance of SelfAttention_v2 to an instance of SelfAttention_v1. To do this, you need to understand the relationship between the weights in both versions. (Hint: nn.Linear stores the weight matrix in a transposed form.) After the assignment, you should observe that both instances produce the same outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aade370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False)\n",
      "Parameter containing:\n",
      "tensor([[-0.1833,  0.2312,  0.4019],\n",
      "        [ 0.1258,  0.0657,  0.1622]], requires_grad=True)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing of nn linear\n",
    "import torch.nn as nn\n",
    "\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "\n",
    "\n",
    "query = nn.Linear(d_in, d_out, bias=False)\n",
    "print(query)\n",
    "print(query.weight)\n",
    "print(50*\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12ffe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors after self-attention:\n",
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Computing attention weights for all tokens\n",
    "import torch\n",
    "from tiny_gpt import TinySelfAttentionQKV, TinySelfAttentionQKVLinear  # Import our tiny self-attention module\n",
    "\n",
    "# -----------------------------\n",
    "# Input sequence embeddings\n",
    "# Each row represents a token in the sequence (e.g., words) and their embedding vectors\n",
    "# For example: \"Your journey starts with one step\"\n",
    "# -----------------------------\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # \"Your\"     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # \"journey\"  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # \"starts\"   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # \"with\"     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # \"one\"      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # \"step\"     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare variables for TinySelfAttentionQKV\n",
    "# d_in: input dimension (size of token embedding)\n",
    "# d_out: output dimension of attention space (can be different from d_in)\n",
    "# Here we use d_out=2 for simplicity\n",
    "# -----------------------------\n",
    "d_in = inputs.shape[1]  # 3, because each token has 3 features\n",
    "d_out = 2               # project into 2-dimensional attention space\n",
    "\n",
    "# -----------------------------\n",
    "# Pick a single token to inspect (optional)\n",
    "# x_2 = inputs[1]  # \"journey\"\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "# Set manual seed for reproducibility\n",
    "# -----------------------------\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# -----------------------------\n",
    "# Instantiate the TinySelfAttentionQKV module\n",
    "# This will create learnable matrices for query, key, and value projections\n",
    "# -----------------------------\n",
    "sa_v1 = TinySelfAttentionQKV(d_in, d_out)\n",
    "\n",
    "# -----------------------------\n",
    "# Forward pass: compute attention output for all tokens\n",
    "# Each token attends to all other tokens and produces a new representation\n",
    "# The output shape is (seq_len, d_out) = (6, 2)\n",
    "# -----------------------------\n",
    "context_vectors = sa_v1(inputs)\n",
    "\n",
    "# Print the context vectors (output of self-attention)\n",
    "print(\"Context vectors after self-attention:\")\n",
    "print(context_vectors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = TinySelfAttentionQKVLinear(d_in, d_out)\n",
    "print(sa_v2(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4e3ce",
   "metadata": {},
   "source": [
    "\n",
    "## Hiding Future Words with Causal Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinygpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
