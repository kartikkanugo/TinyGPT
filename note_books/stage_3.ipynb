{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30ddd3b",
   "metadata": {},
   "source": [
    "# Coding Attention Mechanisms\n",
    "\n",
    "Now we will work on coding attention mechanisms.\n",
    "\n",
    "**Attention** is a mechanism that lets a model decide which parts of the input are most relevant when generating each part of the output.\n",
    "\n",
    "**Self-Attention** is a specific type of attention that allows each position in the input sequence to consider the relevancy of, or “attend to,” all other positions in the same sequence when computing the representation. Self-attention is a key component of contemporary LLMs based on the Transformer architecture, such as the GPT series.\n",
    "\n",
    "## The “Self” in Self-Attention\n",
    "\n",
    "In self-attention, the “self” refers to the mechanism’s ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image.\n",
    "\n",
    "This is in contrast to traditional attention mechanisms, where the focus is on the relationships between elements of two different sequences, such as in sequence-to-sequence models. For example, attention might be computed between an input sequence and an output sequence in translation models.\n",
    "\n",
    "## How Attention Works\n",
    "\n",
    "In a Transformer (the architecture behind LLMs), attention works using three main vectors for each token (word or subword):\n",
    "\n",
    "- **Query (Q)** – What am I looking for?\n",
    "\n",
    "- **Key (K)** – What do I contain?\n",
    "\n",
    "- **Value (V)** – What information do I carry?\n",
    "\n",
    "The model computes how much each word should “attend” to others by comparing the query of the current word with the keys of all other words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ac733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw attention scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "--------------------------------------------------\n",
      "Attention weights (normalized by sum): tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum of weights: tensor(1.0000)\n",
      "--------------------------------------------------\n",
      "Attention weights (naive softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum of weights: tensor(1.)\n",
      "--------------------------------------------------\n",
      "Attention weights (PyTorch softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum of weights: tensor(1.)\n",
      "--------------------------------------------------\n",
      "Context vector (output of attention): tensor([0.4419, 0.6515, 0.5683])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# Input sequence embeddings\n",
    "# Each row represents a token in the sequence (e.g., words) and their embedding vectors\n",
    "# -----------------------------\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # \"Your\"     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # \"journey\"  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # \"starts\"   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # \"with\"     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # \"one\"      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # \"step\"     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Select the query vector\n",
    "# Here, we are computing attention for the second word \"journey\" (x^2)\n",
    "# -----------------------------\n",
    "query = inputs[1]\n",
    "\n",
    "# -----------------------------\n",
    "# Compute raw attention scores\n",
    "# Dot product between the query and each input token\n",
    "# Higher score = more relevant token for this query\n",
    "# -----------------------------\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])  # preallocate tensor for scores\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(\"Raw attention scores:\", attn_scores_2)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Normalize attention scores (simple sum normalization)\n",
    "# Sum of weights = 1 (not ideal, usually softmax is better)\n",
    "# -----------------------------\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights (normalized by sum):\", attn_weights_2_tmp)\n",
    "print(\"Sum of weights:\", attn_weights_2_tmp.sum())\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Define naive softmax function\n",
    "# Converts raw scores into probabilities (weights) between 0 and 1\n",
    "# -----------------------------\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights (naive softmax):\", attn_weights_2_naive)\n",
    "print(\"Sum of weights:\", attn_weights_2_naive.sum())\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Use PyTorch's built-in softmax (numerically stable)\n",
    "# -----------------------------\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights (PyTorch softmax):\", attn_weights_2)\n",
    "print(\"Sum of weights:\", attn_weights_2.sum())\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute context vector\n",
    "# Weighted sum of all input embeddings based on attention weights\n",
    "# This is the output of the self-attention mechanism for the query\n",
    "# -----------------------------\n",
    "context_vec_2 = torch.zeros(query.shape)  # initialize context vector\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(\"Context vector (output of attention):\", context_vec_2)\n",
    "print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535a29e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw attention scores (manual):\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "--------------------------------------------------\n",
      "Raw attention scores (matrix multiplication):\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "--------------------------------------------------\n",
      "Attention weights (softmax):\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "--------------------------------------------------\n",
      "Context vectors (output of self-attention):\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Computing attention weights for all tokens\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# Input sequence embeddings\n",
    "# Each row represents a token in the sequence (e.g., words) and their embedding vectors\n",
    "# -----------------------------\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # \"Your\"     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # \"journey\"  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # \"starts\"   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # \"with\"     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # \"one\"      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # \"step\"     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute raw attention scores manually\n",
    "# attn_scores[i, j] = dot product between token i and token j\n",
    "# This measures how much token i should attend to token j\n",
    "# -----------------------------\n",
    "attn_scores = torch.empty(6, 6)  # initialize 6x6 matrix\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)  # dot product = similarity\n",
    "\n",
    "print(\"Raw attention scores (manual):\")\n",
    "print(attn_scores)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute attention scores using matrix multiplication (faster)\n",
    "# inputs @ inputs.T computes all dot products at once\n",
    "# -----------------------------\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(\"Raw attention scores (matrix multiplication):\")\n",
    "print(attn_scores)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "# Apply softmax along each row\n",
    "# Converts raw scores into attention weights (probabilities) for each token\n",
    "# Each row sums to 1\n",
    "# -----------------------------\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(\"Attention weights (softmax):\")\n",
    "print(attn_weights)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# Compute context vectors for all tokens\n",
    "# Multiply attention weights with input embeddings\n",
    "# This gives the new representation of each token after attending to all tokens\n",
    "# -----------------------------\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(\"Context vectors (output of self-attention):\")\n",
    "print(all_context_vecs)\n",
    "print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d29864",
   "metadata": {},
   "source": [
    "### THE RATIONALE BEHIND SCALED-DOT PRODUCT ATTENTION\n",
    "\n",
    "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "\n",
    "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention.\n",
    "\n",
    "**Look at implementation of TinySelfAttentionQKV**\n",
    "\n",
    "Note that nn.Linear in SelfAttention_v2 uses a different weight initialization scheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1, which causes both mechanisms to produce different results. To check that both implementations, SelfAttention_v1 and SelfAttention_v2, are otherwise similar, we can transfer the weight matrices from a SelfAttention_v2 object to a SelfAttention_v1, such that both objects then produce the same results.\n",
    "\n",
    "Your task is to correctly assign the weights from an instance of SelfAttention_v2 to an instance of SelfAttention_v1. To do this, you need to understand the relationship between the weights in both versions. (Hint: nn.Linear stores the weight matrix in a transposed form.) After the assignment, you should observe that both instances produce the same outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aade370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False)\n",
      "Parameter containing:\n",
      "tensor([[-0.2604,  0.1829, -0.2569],\n",
      "        [ 0.4126,  0.4611, -0.5323]], requires_grad=True)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing of nn linear\n",
    "import torch.nn as nn\n",
    "\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "\n",
    "\n",
    "query = nn.Linear(d_in, d_out, bias=False)\n",
    "print(query)\n",
    "print(query.weight)\n",
    "print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12ffe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors after self-attention:\n",
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Computing attention weights for all tokens\n",
    "import torch\n",
    "from tiny_gpt import (\n",
    "    TinySelfAttentionQKV,\n",
    "    TinySelfAttentionQKVLinear,\n",
    ")  # Import our tiny self-attention module\n",
    "\n",
    "# -----------------------------\n",
    "# Input sequence embeddings\n",
    "# Each row represents a token in the sequence (e.g., words) and their embedding vectors\n",
    "# For example: \"Your journey starts with one step\"\n",
    "# -----------------------------\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # \"Your\"     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # \"journey\"  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # \"starts\"   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # \"with\"     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # \"one\"      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # \"step\"     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare variables for TinySelfAttentionQKV\n",
    "# d_in: input dimension (size of token embedding)\n",
    "# d_out: output dimension of attention space (can be different from d_in)\n",
    "# Here we use d_out=2 for simplicity\n",
    "# -----------------------------\n",
    "d_in = inputs.shape[1]  # 3, because each token has 3 features\n",
    "d_out = 2  # project into 2-dimensional attention space\n",
    "\n",
    "# -----------------------------\n",
    "# Pick a single token to inspect (optional)\n",
    "# x_2 = inputs[1]  # \"journey\"\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "# Set manual seed for reproducibility\n",
    "# -----------------------------\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# -----------------------------\n",
    "# Instantiate the TinySelfAttentionQKV module\n",
    "# This will create learnable matrices for query, key, and value projections\n",
    "# -----------------------------\n",
    "sa_v1 = TinySelfAttentionQKV(d_in, d_out)\n",
    "\n",
    "# -----------------------------\n",
    "# Forward pass: compute attention output for all tokens\n",
    "# Each token attends to all other tokens and produces a new representation\n",
    "# The output shape is (seq_len, d_out) = (6, 2)\n",
    "# -----------------------------\n",
    "context_vectors = sa_v1(inputs)\n",
    "\n",
    "# Print the context vectors (output of self-attention)\n",
    "print(\"Context vectors after self-attention:\")\n",
    "print(context_vectors)\n",
    "\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = TinySelfAttentionQKVLinear(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4e3ce",
   "metadata": {},
   "source": [
    "## Hiding Future Words with Causal Attention\n",
    "\n",
    "Causal attention, also known as masked attention, is a specialized form of self-attention. It restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dec9686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tiny_gpt import TinySelfAttentionQKV, TinySelfAttentionQKVLinear\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # \"Your\"     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # \"journey\"  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # \"starts\"   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # \"with\"     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # \"one\"      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # \"step\"     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = TinySelfAttentionQKVLinear(d_in, d_out)\n",
    "print(sa_v2(inputs))\n",
    "\n",
    "queries = sa_v2.w_query(inputs)\n",
    "keys = sa_v2.w_key(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df62b38",
   "metadata": {},
   "source": [
    "**Attention weights** = how much each input element should contribute to the current output.\n",
    "\n",
    "For every token there is a attention weight, that tells how much it affects the other words.(sort off)\n",
    "\n",
    "To hide the future words, attention weights of future words should be zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fad0468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tril functions ... tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "--------------------------------------------------\n",
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "--------------------------------------------------\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "--------------------------------------------------\n",
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "--------------------------------------------------\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "--------------------------------------------------\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "context_length = attention_scores.shape[0]\n",
    "\n",
    "# tril lower triangular matrix\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "\n",
    "print(f\"tril functions ... {mask_simple}\")\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "# triu upper triangular matrix\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)\n",
    "print(50 * \"-\")\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "print(attn_weights)\n",
    "print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540f587",
   "metadata": {},
   "source": [
    "### Masking additional attention weights with dropout\n",
    "\n",
    "Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively “dropping” them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It’s important to emphasize that dropout is only used during training and is disabled afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66407036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "--------------------------------------------------\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)  # 1\n",
    "example = torch.ones(6, 6)  # 2\n",
    "print(dropout(example))\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))\n",
    "print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4063b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tiny_gpt import TinyCausalAttention\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # \"Your\"     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # \"journey\"  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # \"starts\"   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # \"with\"     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # \"one\"      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # \"step\"     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Batch technically is the output actually like input and targets from dataloader... this is just a test class\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)\n",
    "\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = TinyCausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc2051",
   "metadata": {},
   "source": [
    "### Multi Head Attention\n",
    "\n",
    "The term “multi-head” refers to dividing the attention mechanism into multiple “heads,” each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.\n",
    "\n",
    "We will tackle this expansion from causal attention to multi-head attention. First, we will intuitively build a multi-head attention module by stacking multiple CausalAttention modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b5098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n",
      "--------------------------------------------------\n",
      "tensor([[[[0.2745, 0.8993, 0.7179],\n",
      "          [0.6584, 0.0390, 0.7058],\n",
      "          [0.2775, 0.9268, 0.9156],\n",
      "          [0.8573, 0.7388, 0.4340]],\n",
      "\n",
      "         [[0.0772, 0.4066, 0.4606],\n",
      "          [0.3565, 0.2318, 0.5159],\n",
      "          [0.1479, 0.4545, 0.4220],\n",
      "          [0.5331, 0.9737, 0.5786]]]])\n",
      "--------------------------------------------------\n",
      "torch.Size([1, 2, 4, 3])\n",
      "--------------------------------------------------\n",
      "tensor([[0.2745, 0.6584, 0.2775, 0.8573],\n",
      "        [0.8993, 0.0390, 0.9268, 0.7388],\n",
      "        [0.7179, 0.7058, 0.9156, 0.4340]])\n",
      "torch.Size([3, 4])\n",
      "--------------------------------------------------\n",
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\\n tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [0.2745, 0.6584, 0.2775, 0.8573],  # 1\n",
    "                [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                [0.7179, 0.7058, 0.9156, 0.4340],\n",
    "            ],\n",
    "            [\n",
    "                [0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                [0.4606, 0.5159, 0.4220, 0.5786],\n",
    "            ],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(a.shape)\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "b = a.transpose(2, 3)\n",
    "print(b)\n",
    "print(50 * \"-\")\n",
    "\n",
    "print(b.shape)\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "first_head = a[0, 0, :, :]\n",
    "print(first_head)\n",
    "print(first_head.shape)\n",
    "print(50 * \"-\")\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b368c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "--------------------------------------------------\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "--------------------------------------------------\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tiny_gpt import TinyMultiHeadAttention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # \"Your\"     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # \"journey\"  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # \"starts\"   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # \"with\"     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # \"one\"      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # \"step\"     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Batch technically is the output actually like input and targets from dataloader... this is just a test class\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)\n",
    "print(50 * \"-\")\n",
    "\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = TinyMultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(50 * \"-\")\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583469d",
   "metadata": {},
   "source": [
    "For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention heads and a context vector embedding size of 768. The largest GPT-2 model (1.5 billion parameters) has 25 attention heads and a context vector embedding size of 1,600. The embedding sizes of the token inputs and context embeddings are the same in GPT models (d_in = d_out)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537b6a9",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Attention mechanisms transform input elements into enhanced context vector representations that incorporate information about all inputs.\n",
    "- A self-attention mechanism computes the context vector representation as a weighted sum over the inputs.\n",
    "- In a simplified attention mechanism, the attention weights are computed via dot products.\n",
    "- A dot product is a concise way of multiplying two vectors element-wise and then summing the products.\n",
    "- Matrix multiplications, while not strictly required, help us implement computations more efficiently and compactly by replacing nested for loops.\n",
    "- In self-attention mechanisms used in LLMs, also called scaled-dot product attention, we include trainable weight matrices to compute intermediate transformations of the inputs: queries, values, and keys.\n",
    "- When working with LLMs that read and generate text from left to right, we add a causal attention mask to prevent the LLM from accessing future tokens.\n",
    "- In addition to causal attention masks to zero-out attention weights, we can add a dropout mask to reduce overfitting in LLMs.\n",
    "- The attention modules in transformer-based LLMs involve multiple instances of causal attention, which is called multi-head attention.\n",
    "- We can create a multi-head attention module by stacking multiple instances of causal attention modules.\n",
    "- A more efficient way of creating multi-head attention modules involves batched matrix multiplications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinygpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
