{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258b05ea",
   "metadata": {},
   "source": [
    "# Stage 2 - Tokenization and Data Piplines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379d5921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stage 1\n",
      "==================================================\n",
      "Running env_display_tinygpt_modules\n",
      "üîß Environment Summary\n",
      "--------------------------------------------------\n",
      "Python version : 3.13.7\n",
      "Platform       : Windows 11\n",
      "\n",
      "Torch version  : 2.9.0+cu130\n",
      "NumPy version  : 2.3.4\n",
      "Tiktoken ver.  : 0.12.0\n",
      "Pandas version : 2.3.3\n",
      "--------------------------------------------------\n",
      "‚öôÔ∏è  CUDA & GPU Information\n",
      "--------------------------------------------------\n",
      "‚úÖ CUDA available: 13.0\n",
      "üß† GPU name      : NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "üíΩ Total memory  : 8.59 GB\n",
      "--------------------------------------------------\n",
      "üî§ Tokenizer Test (tiktoken)\n",
      "--------------------------------------------------\n",
      "Input text : Once upon a time in TinyGPT...\n",
      "Tokens     : [7454, 2402, 257, 640, 287, 20443, 38, 11571, 986]\n",
      "Decoded    : Once upon a time in TinyGPT...\n",
      "--------------------------------------------------\n",
      "‚úÖ TinyGPT environment initialized successfully!\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "Running io_load_text_file\n",
      "Loaded '../The_Verdict.txt' successfully!\n",
      "Total number of characters: 20479\n",
      "Preview (first 100 chars):\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n",
      "==================================================\n",
      "Stage 1 Successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "# Import previous stages\n",
    "from tiny_gpt import env_display_tinygpt_modules, io_load_text_file\n",
    "print(\"Running Stage 1\")\n",
    "print(50* \"=\")\n",
    "\n",
    "\n",
    "env_display_tinygpt_modules()\n",
    "\n",
    "print(50* \"=\")\n",
    "raw_text_data = io_load_text_file()\n",
    "\n",
    "\n",
    "print(50* \"=\")\n",
    "print(\"Stage 1 Successfully Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f2d86",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A ‚Äì Tokenization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ffef7",
   "metadata": {},
   "source": [
    "- Words are represented as continuous-valued vectors, allowing mathematical operations to be applied on them.\n",
    "\n",
    "- The process of converting text into vector form is known as embedding.\n",
    "\n",
    "- Retrieval-Augmented Generation (RAG) combines two steps:\n",
    "\n",
    "  - Retrieval ‚Äî searching an external knowledge base for relevant information.\n",
    "\n",
    "  - Generation ‚Äî producing coherent text using both the retrieved and contextual information.\n",
    "\n",
    "- The embedding size (or dimensionality) represents the number of features in each word vector ‚Äî it defines the hidden state size of the model.\n",
    "\n",
    "- For instance, the GPT-2 model uses 768-dimensional embeddings, meaning each word is encoded across 768 distinct numerical features.\n",
    "  Think of it as measuring a word‚Äôs meaning along many abstract axes ‚Äî for example, a ‚Äúbird‚Äù might score high on a ‚Äòcan-fly‚Äô dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407da79",
   "metadata": {},
   "source": [
    "### Regex Tokenizer Concept\n",
    "- Using regular expressions build a tokenizer\n",
    "- We also refrain from making all text lowercase because capitalization helps LLMs distinguish between proper nouns and common nouns, understand sentence structure, and learn to generate text with proper capitalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325f4174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© Tokenized Vocabulary List:\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--', 'deploring', 'his', 'unaccountable', 'abdication', '.', '\"', 'Of', 'course', 'it', \"'\", 's', 'going', 'to', 'send', 'the', 'value', 'of', 'my', 'picture', \"'\", 'way', 'up', ';', 'but', 'I', 'don', \"'\", 't', 'think', 'of', 'that', ',', 'Mr', '.', 'Rickham', '--', 'the', 'loss', 'to', 'Arrt', 'is', 'all', 'I', 'think', 'of', '.', '\"', 'The', 'word', ',', 'on', 'Mrs', '.', 'Thwing', \"'\", 's', 'lips', ',', 'multiplied', 'its', '_', 'rs', '_', 'as', 'though', 'they', 'were', 'reflected', 'in', 'an', 'endless', 'vista', 'of', 'mirrors', '.', 'And', 'it', 'was', 'not', 'only', 'the', 'Mrs', '.', 'Thwings', 'who', 'mourned', '.', 'Had', 'not', 'the', 'exquisite', 'Hermia', 'Croft', ',', 'at', 'the', 'last', 'Grafton', 'Gallery', 'show', ',', 'stopped', 'me', 'before', 'Gisburn', \"'\", 's', '\"', 'Moon-dancers', '\"', 'to', 'say', ',', 'with', 'tears', 'in', 'her', 'eyes', ':', '\"', 'We', 'shall', 'not', 'look', 'upon', 'its', 'like', 'again', '\"', '?', 'Well', '!', '--', 'even', 'through', 'the', 'prism', 'of', 'Hermia', \"'\", 's', 'tears', 'I', 'felt', 'able', 'to', 'face', 'the', 'fact', 'with', 'equanimity', '.', 'Poor', 'Jack', 'Gisburn', '!', 'The', 'women', 'had', 'made', 'him', '--', 'it', 'was', 'fitting', 'that', 'they', 'should', 'mourn', 'him', '.', 'Among', 'his', 'own', 'sex', 'fewer', 'regrets', 'were', 'heard', ',', 'and', 'in', 'his', 'own', 'trade', 'hardly', 'a', 'murmur', '.', 'Professional', 'jealousy', '?', 'Perhaps', '.', 'If', 'it', 'were', ',', 'the', 'honour', 'of', 'the', 'craft', 'was', 'vindicated', 'by', 'little', 'Claude', 'Nutley', ',', 'who', ',', 'in', 'all', 'good', 'faith', ',', 'brought', 'out', 'in', 'the', 'Burlington', 'a', 'very', 'handsome', '\"', 'obituary', '\"', 'on', 'Jack', '--', 'one', 'of', 'those', 'showy', 'articles', 'stocked', 'with', 'random', 'technicalities', 'that', 'I', 'have', 'heard', '(', 'I', 'won', \"'\", 't', 'say', 'by', 'whom', ')', 'compared', 'to', 'Gisburn', \"'\", 's', 'painting', '.', 'And', 'so', '--', 'his', 'resolve', 'being', 'apparently', 'irrevocable', '--', 'the', 'discussion', 'gradually', 'died', 'out', ',', 'and', ',', 'as', 'Mrs', '.', 'Thwing', 'had', 'predicted', ',', 'the', 'price', 'of', '\"', 'Gisburns', '\"', 'went', 'up', '.', 'It', 'was', 'not', 'till', 'three', 'years', 'later', 'that', ',', 'in', 'the', 'course', 'of', 'a', 'few', 'weeks', \"'\", 'idling', 'on', 'the', 'Riviera', ',', 'it', 'suddenly', 'occurred', 'to', 'me', 'to', 'wonder', 'why', 'Gisburn', 'had', 'given', 'up', 'his', 'painting', '.', 'On', 'reflection', ',', 'it', 'really', 'was', 'a', 'tempting', 'problem', '.', 'To', 'accuse', 'his', 'wife', 'would', 'have', 'been', 'too', 'easy', '--', 'his', 'fair', 'sitters', 'had', 'been', 'denied', 'the', 'solace', 'of', 'saying', 'that', 'Mrs', '.', 'Gisburn', 'had', '\"', 'dragged', 'him', 'down', '.', '\"', 'For', 'Mrs', '.', 'Gisburn', '--', 'as', 'such', '--', 'had', 'not', 'existed', 'till', 'nearly', 'a', 'year', 'after', 'Jack', \"'\", 's', 'resolve', 'had', 'been', 'taken', '.', 'It', 'might', 'be', 'that', 'he', 'had', 'married', 'her', '--', 'since', 'he', 'liked', 'his', 'ease', '--', 'because', 'he', 'didn', \"'\", 't', 'want', 'to', 'go', 'on', 'painting', ';', 'but', 'it', 'would', 'have', 'been', 'hard', 'to', 'prove', 'that', 'he', 'had', 'given', 'up', 'his', 'painting', 'because', 'he', 'had', 'married', 'her', '.', 'Of', 'course', ',', 'if', 'she', 'had', 'not', 'dragged', 'him', 'down', ',', 'she', 'had', 'equally', ',', 'as', 'Miss', 'Croft', 'contended', ',', 'failed', 'to', '\"', 'lift', 'him', 'up', '\"', '--', 'she', 'had', 'not', 'led', 'him', 'back', 'to', 'the', 'easel', '.', 'To', 'put', 'the', 'brush', 'into', 'his', 'hand', 'again', '--', 'what', 'a', 'vocation', 'for', 'a', 'wife', '!', 'But', 'Mrs', '.', 'Gisburn', 'appeared', 'to', 'have', 'disdained', 'it', '--', 'and', 'I', 'felt', 'it', 'might', 'be', 'interesting', 'to', 'find', 'out', 'why', '.', 'The', 'desultory', 'life', 'of', 'the', 'Riviera', 'lends', 'itself', 'to', 'such', 'purely', 'academic', 'speculations', ';', 'and', 'having', ',', 'on', 'my', 'way', 'to', 'Monte', 'Carlo', ',', 'caught', 'a', 'glimpse', 'of', 'Jack', \"'\", 's', 'balustraded', 'terraces', 'between', 'the', 'pines', ',', 'I', 'had', 'myself', 'borne', 'thither', 'the', 'next', 'day', '.', 'I', 'found', 'the', 'couple', 'at', 'tea', 'beneath', 'their', 'palm-trees', ';', 'and', 'Mrs', '.', 'Gisburn', \"'\", 's', 'welcome', 'was', 'so', 'genial', 'that', ',', 'in', 'the', 'ensuing', 'weeks', ',', 'I', 'claimed', 'it', 'frequently', '.', 'It', 'was', 'not', 'that', 'my', 'hostess', 'was', '\"', 'interesting', '\"', ':', 'on', 'that', 'point', 'I', 'could', 'have', 'given', 'Miss', 'Croft', 'the', 'fullest', 'reassurance', '.', 'It', 'was', 'just', 'because', 'she', 'was', '_', 'not', '_', 'interesting', '--', 'if', 'I', 'may', 'be', 'pardoned', 'the', 'bull', '--', 'that', 'I', 'found', 'her', 'so', '.', 'For', 'Jack', ',', 'all', 'his', 'life', ',', 'had', 'been', 'surrounded', 'by', 'interesting', 'women', ':', 'they', 'had', 'fostered', 'his', 'art', ',', 'it', 'had', 'been', 'reared', 'in', 'the', 'hot-house', 'of', 'their', 'adulation', '.', 'And', 'it', 'was', 'therefore', 'instructive', 'to', 'note', 'what', 'effect', 'the', '\"', 'deadening', 'atmosphere', 'of', 'mediocrity', '\"', '(', 'I', 'quote', 'Miss', 'Croft', ')', 'was', 'having', 'on', 'him', '.', 'I', 'have', 'mentioned', 'that', 'Mrs', '.', 'Gisburn', 'was', 'rich', ';', 'and', 'it', 'was', 'immediately', 'perceptible', 'that', 'her', 'husband', 'was', 'extracting', 'from', 'this', 'circumstance', 'a', 'delicate', 'but', 'substantial', 'satisfaction', '.', 'It', 'is', ',', 'as', 'a', 'rule', ',', 'the', 'people', 'who', 'scorn', 'money', 'who', 'get', 'most', 'out', 'of', 'it', ';', 'and', 'Jack', \"'\", 's', 'elegant', 'disdain', 'of', 'his', 'wife', \"'\", 's', 'big', 'balance', 'enabled', 'him', ',', 'with', 'an', 'appearance', 'of', 'perfect', 'good-breeding', ',', 'to', 'transmute', 'it', 'into', 'objects', 'of', 'art', 'and', 'luxury', '.', 'To', 'the', 'latter', ',', 'I', 'must', 'add', ',', 'he', 'remained', 'relatively', 'indifferent', ';', 'but', 'he', 'was', 'buying', 'Renaissance', 'bronzes', 'and', 'eighteenth-century', 'pictures', 'with', 'a', 'discrimination', 'that', 'bespoke', 'the', 'amplest', 'resources', '.', '\"', 'Money', \"'\", 's', 'only', 'excuse', 'is', 'to', 'put', 'beauty', 'into', 'circulation', ',', '\"', 'was', 'one', 'of', 'the', 'axioms', 'he', 'laid', 'down', 'across', 'the', 'Sevres', 'and', 'silver', 'of', 'an', 'exquisitely', 'appointed', 'luncheon-table', ',', 'when', ',', 'on', 'a', 'later', 'day', ',', 'I', 'had', 'again', 'run', 'over', 'from', 'Monte', 'Carlo', ';', 'and', 'Mrs', '.', 'Gisburn', ',', 'beaming', 'on', 'him', ',', 'added', 'for', 'my', 'enlightenment', ':', '\"', 'Jack', 'is', 'so', 'morbidly', 'sensitive', 'to', 'every', 'form', 'of', 'beauty', '.', '\"', 'Poor', 'Jack', '!', 'It', 'had', 'always', 'been', 'his', 'fate', 'to', 'have', 'women', 'say', 'such', 'things', 'of', 'him', ':', 'the', 'fact', 'should', 'be', 'set', 'down', 'in', 'extenuation', '.', 'What', 'struck', 'me', 'now', 'was', 'that', ',', 'for', 'the', 'first', 'time', ',', 'he', 'resented', 'the', 'tone', '.', 'I', 'had', 'seen', 'him', ',', 'so', 'often', ',', 'basking', 'under', 'similar', 'tributes', '--', 'was', 'it', 'the', 'conjugal', 'note', 'that', 'robbed', 'them', 'of', 'their', 'savour', '?', 'No', '--', 'for', ',', 'oddly', 'enough', ',', 'it', 'became', 'apparent', 'that', 'he', 'was', 'fond', 'of', 'Mrs', '.', 'Gisburn', '--', 'fond', 'enough', 'not', 'to', 'see', 'her', 'absurdity', '.', 'It', 'was', 'his', 'own', 'absurdity', 'he', 'seemed', 'to', 'be', 'wincing', 'under', '--', 'his', 'own', 'attitude', 'as', 'an', 'object', 'for', 'garlands', 'and', 'incense', '.', '\"', 'My', 'dear', ',', 'since', 'I', \"'\", 've', 'chucked', 'painting', 'people', 'don', \"'\", 't', 'say', 'that', 'stuff', 'about', 'me', '--', 'they', 'say', 'it', 'about', 'Victor', 'Grindle', ',', '\"', 'was', 'his', 'only', 'protest', ',', 'as', 'he', 'rose', 'from', 'the', 'table', 'and', 'strolled', 'out', 'onto', 'the', 'sunlit', 'terrace', '.', 'I', 'glanced', 'after', 'him', ',', 'struck', 'by', 'his', 'last', 'word', '.', 'Victor', 'Grindle', 'was', ',', 'in', 'fact', ',', 'becoming', 'the', 'man', 'of', 'the', 'moment', '--', 'as', 'Jack', 'himself', ',', 'one', 'might', 'put', 'it', ',', 'had', 'been', 'the', 'man', 'of', 'the', 'hour', '.', 'The', 'younger', 'artist', 'was', 'said', 'to', 'have', 'formed', 'himself', 'at', 'my', 'friend', \"'\", 's', 'feet', ',', 'and', 'I', 'wondered', 'if', 'a', 'tinge', 'of', 'jealousy', 'underlay', 'the', 'latter', \"'\", 's', 'mysterious', 'abdication', '.', 'But', 'no', '--', 'for', 'it', 'was', 'not', 'till', 'after', 'that', 'event', 'that', 'the', '_', 'rose', 'Dubarry', '_', 'drawing-rooms', 'had', 'begun', 'to', 'display', 'their', '\"', 'Grindles', '.', '\"', 'I', 'turned', 'to', 'Mrs', '.', 'Gisburn', ',', 'who', 'had', 'lingered', 'to', 'give', 'a', 'lump', 'of', 'sugar', 'to', 'her', 'spaniel', 'in', 'the', 'dining-room', '.', '\"', 'Why', '_', 'has', '_', 'he', 'chucked', 'painting', '?', '\"', 'I', 'asked', 'abruptly', '.', 'She', 'raised', 'her', 'eyebrows', 'with', 'a', 'hint', 'of', 'good-humoured', 'surprise', '.', '\"', 'Oh', ',', 'he', 'doesn', \"'\", 't', '_', 'have', '_', 'to', 'now', ',', 'you', 'know', ';', 'and', 'I', 'want', 'him', 'to', 'enjoy', 'himself', ',', '\"', 'she', 'said', 'quite', 'simply', '.', 'I', 'looked', 'about', 'the', 'spacious', 'white-panelled', 'room', ',', 'with', 'its', '_', 'famille-verte', '_', 'vases', 'repeating', 'the', 'tones', 'of', 'the', 'pale', 'damask', 'curtains', ',', 'and', 'its', 'eighteenth-century', 'pastels', 'in', 'delicate', 'faded', 'frames', '.', '\"', 'Has', 'he', 'chucked', 'his', 'pictures', 'too', '?', 'I', 'haven', \"'\", 't', 'seen', 'a', 'single', 'one', 'in', 'the', 'house', '.', '\"', 'A', 'slight', 'shade', 'of', 'constraint', 'crossed', 'Mrs', '.', 'Gisburn', \"'\", 's', 'open', 'countenance', '.', '\"', 'It', \"'\", 's', 'his', 'ridiculous', 'modesty', ',', 'you', 'know', '.', 'He', 'says', 'they', \"'\", 're', 'not', 'fit', 'to', 'have', 'about', ';', 'he', \"'\", 's', 'sent', 'them', 'all', 'away', 'except', 'one', '--', 'my', 'portrait', '--', 'and', 'that', 'I', 'have', 'to', 'keep', 'upstairs', '.', '\"', 'His', 'ridiculous', 'modesty', '--', 'Jack', \"'\", 's', 'modesty', 'about', 'his', 'pictures', '?', 'My', 'curiosity', 'was', 'growing', 'like', 'the', 'bean-stalk', '.', 'I', 'said', 'persuasively', 'to', 'my', 'hostess', ':', '\"', 'I', 'must', 'really', 'see', 'your', 'portrait', ',', 'you', 'know', '.', '\"', 'She', 'glanced', 'out', 'almost', 'timorously', 'at', 'the', 'terrace', 'where', 'her', 'husband', ',', 'lounging', 'in', 'a', 'hooded', 'chair', ',', 'had', 'lit', 'a', 'cigar', 'and', 'drawn', 'the', 'Russian', 'deerhound', \"'\", 's', 'head', 'between', 'his', 'knees', '.', '\"', 'Well', ',', 'come', 'while', 'he', \"'\", 's', 'not', 'looking', ',', '\"', 'she', 'said', ',', 'with', 'a', 'laugh', 'that', 'tried', 'to', 'hide', 'her', 'nervousness', ';', 'and', 'I', 'followed', 'her', 'between', 'the', 'marble', 'Emperors', 'of', 'the', 'hall', ',', 'and', 'up', 'the', 'wide', 'stairs', 'with', 'terra-cotta', 'nymphs', 'poised', 'among', 'flowers', 'at', 'each', 'landing', '.', 'In', 'the', 'dimmest', 'corner', 'of', 'her', 'boudoir', ',', 'amid', 'a', 'profusion', 'of', 'delicate', 'and', 'distinguished', 'objects', ',', 'hung', 'one', 'of', 'the', 'familiar', 'oval', 'canvases', ',', 'in', 'the', 'inevitable', 'garlanded', 'frame', '.', 'The', 'mere', 'outline', 'of', 'the', 'frame', 'called', 'up', 'all', 'Gisburn', \"'\", 's', 'past', '!', 'Mrs', '.', 'Gisburn', 'drew', 'back', 'the', 'window-curtains', ',', 'moved', 'aside', 'a', '_', 'jardiniere', '_', 'full', 'of', 'pink', 'azaleas', ',', 'pushed', 'an', 'arm-chair', 'away', ',', 'and', 'said', ':', '\"', 'If', 'you', 'stand', 'here', 'you', 'can', 'just', 'manage', 'to', 'see', 'it', '.', 'I', 'had', 'it', 'over', 'the', 'mantel-piece', ',', 'but', 'he', 'wouldn', \"'\", 't', 'let', 'it', 'stay', '.', '\"', 'Yes', '--', 'I', 'could', 'just', 'manage', 'to', 'see', 'it', '--', 'the', 'first', 'portrait', 'of', 'Jack', \"'\", 's', 'I', 'had', 'ever', 'had', 'to', 'strain', 'my', 'eyes', 'over', '!', 'Usually', 'they', 'had', 'the', 'place', 'of', 'honour', '--', 'say', 'the', 'central', 'panel', 'in', 'a', 'pale', 'yellow', 'or', '_', 'rose', 'Dubarry', '_', 'drawing-room', ',', 'or', 'a', 'monumental', 'easel', 'placed', 'so', 'that', 'it', 'took', 'the', 'light', 'through', 'curtains', 'of', 'old', 'Venetian', 'point', '.', 'The', 'more', 'modest', 'place', 'became', 'the', 'picture', 'better', ';', 'yet', ',', 'as', 'my', 'eyes', 'grew', 'accustomed', 'to', 'the', 'half-light', ',', 'all', 'the', 'characteristic', 'qualities', 'came', 'out', '--', 'all', 'the', 'hesitations', 'disguised', 'as', 'audacities', ',', 'the', 'tricks', 'of', 'prestidigitation', 'by', 'which', ',', 'with', 'such', 'consummate', 'skill', ',', 'he', 'managed', 'to', 'divert', 'attention', 'from', 'the', 'real', 'business', 'of', 'the', 'picture', 'to', 'some', 'pretty', 'irrelevance', 'of', 'detail', '.', 'Mrs', '.', 'Gisburn', ',', 'presenting', 'a', 'neutral', 'surface', 'to', 'work', 'on', '--', 'forming', ',', 'as', 'it', 'were', ',', 'so', 'inevitably', 'the', 'background', 'of', 'her', 'own', 'picture', '--', 'had', 'lent', 'herself', 'in', 'an', 'unusual', 'degree', 'to', 'the', 'display', 'of', 'this', 'false', 'virtuosity', '.', 'The', 'picture', 'was', 'one', 'of', 'Jack', \"'\", 's', '\"', 'strongest', ',', '\"', 'as', 'his', 'admirers', 'would', 'have', 'put', 'it', '--', 'it', 'represented', ',', 'on', 'his', 'part', ',', 'a', 'swelling', 'of', 'muscles', ',', 'a', 'congesting', 'of', 'veins', ',', 'a', 'balancing', ',', 'straddling', 'and', 'straining', ',', 'that', 'reminded', 'one', 'of', 'the', 'circus-clown', \"'\", 's', 'ironic', 'efforts', 'to', 'lift', 'a', 'feather', '.', 'It', 'met', ',', 'in', 'short', ',', 'at', 'every', 'point', 'the', 'demand', 'of', 'lovely', 'woman', 'to', 'be', 'painted', '\"', 'strongly', '\"', 'because', 'she', 'was', 'tired', 'of', 'being', 'painted', '\"', 'sweetly', '\"', '--', 'and', 'yet', 'not', 'to', 'lose', 'an', 'atom', 'of', 'the', 'sweetness', '.', '\"', 'It', \"'\", 's', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.', '\"', 'The', 'last', 'but', 'one', ',', '\"', 'she', 'corrected', 'herself', '--', '\"', 'but', 'the', 'other', 'doesn', \"'\", 't', 'count', ',', 'because', 'he', 'destroyed', 'it', '.', '\"', '\"', 'Destroyed', 'it', '?', '\"', 'I', 'was', 'about', 'to', 'follow', 'up', 'this', 'clue', 'when', 'I', 'heard', 'a', 'footstep', 'and', 'saw', 'Jack', 'himself', 'on', 'the', 'threshold', '.', 'As', 'he', 'stood', 'there', ',', 'his', 'hands', 'in', 'the', 'pockets', 'of', 'his', 'velveteen', 'coat', ',', 'the', 'thin', 'brown', 'waves', 'of', 'hair', 'pushed', 'back', 'from', 'his', 'white', 'forehead', ',', 'his', 'lean', 'sunburnt', 'cheeks', 'furrowed', 'by', 'a', 'smile', 'that', 'lifted', 'the', 'tips', 'of', 'a', 'self-confident', 'moustache', ',', 'I', 'felt', 'to', 'what', 'a', 'degree', 'he', 'had', 'the', 'same', 'quality', 'as', 'his', 'pictures', '--', 'the', 'quality', 'of', 'looking', 'cleverer', 'than', 'he', 'was', '.', 'His', 'wife', 'glanced', 'at', 'him', 'deprecatingly', ',', 'but', 'his', 'eyes', 'travelled', 'past', 'her', 'to', 'the', 'portrait', '.', '\"', 'Mr', '.', 'Rickham', 'wanted', 'to', 'see', 'it', ',', '\"', 'she', 'began', ',', 'as', 'if', 'excusing', 'herself', '.', 'He', 'shrugged', 'his', 'shoulders', ',', 'still', 'smiling', '.', '\"', 'Oh', ',', 'Rickham', 'found', 'me', 'out', 'long', 'ago', ',', '\"', 'he', 'said', 'lightly', ';', 'then', ',', 'passing', 'his', 'arm', 'through', 'mine', ':', '\"', 'Come', 'and', 'see', 'the', 'rest', 'of', 'the', 'house', '.', '\"', 'He', 'showed', 'it', 'to', 'me', 'with', 'a', 'kind', 'of', 'naive', 'suburban', 'pride', ':', 'the', 'bath-rooms', ',', 'the', 'speaking-tubes', ',', 'the', 'dress-closets', ',', 'the', 'trouser-presses', '--', 'all', 'the', 'complex', 'simplifications', 'of', 'the', 'millionaire', \"'\", 's', 'domestic', 'economy', '.', 'And', 'whenever', 'my', 'wonder', 'paid', 'the', 'expected', 'tribute', 'he', 'said', ',', 'throwing', 'out', 'his', 'chest', 'a', 'little', ':', '\"', 'Yes', ',', 'I', 'really', 'don', \"'\", 't', 'see', 'how', 'people', 'manage', 'to', 'live', 'without', 'that', '.', '\"', 'Well', '--', 'it', 'was', 'just', 'the', 'end', 'one', 'might', 'have', 'foreseen', 'for', 'him', '.', 'Only', 'he', 'was', ',', 'through', 'it', 'all', 'and', 'in', 'spite', 'of', 'it', 'all', '--', 'as', 'he', 'had', 'been', 'through', ',', 'and', 'in', 'spite', 'of', ',', 'his', 'pictures', '--', 'so', 'handsome', ',', 'so', 'charming', ',', 'so', 'disarming', ',', 'that', 'one', 'longed', 'to', 'cry', 'out', ':', '\"', 'Be', 'dissatisfied', 'with', 'your', 'leisure', '!', '\"', 'as', 'once', 'one', 'had', 'longed', 'to', 'say', ':', '\"', 'Be', 'dissatisfied', 'with', 'your', 'work', '!', '\"', 'But', ',', 'with', 'the', 'cry', 'on', 'my', 'lips', ',', 'my', 'diagnosis', 'suffered', 'an', 'unexpected', 'check', '.', '\"', 'This', 'is', 'my', 'own', 'lair', ',', '\"', 'he', 'said', ',', 'leading', 'me', 'into', 'a', 'dark', 'plain', 'room', 'at', 'the', 'end', 'of', 'the', 'florid', 'vista', '.', 'It', 'was', 'square', 'and', 'brown', 'and', 'leathery', ':', 'no', '\"', 'effects', '\"', ';', 'no', 'bric-a-brac', ',', 'none', 'of', 'the', 'air', 'of', 'posing', 'for', 'reproduction', 'in', 'a', 'picture', 'weekly', '--', 'above', 'all', ',', 'no', 'least', 'sign', 'of', 'ever', 'having', 'been', 'used', 'as', 'a', 'studio', '.', 'The', 'fact', 'brought', 'home', 'to', 'me', 'the', 'absolute', 'finality', 'of', 'Jack', \"'\", 's', 'break', 'with', 'his', 'old', 'life', '.', '\"', 'Don', \"'\", 't', 'you', 'ever', 'dabble', 'with', 'paint', 'any', 'more', '?', '\"', 'I', 'asked', ',', 'still', 'looking', 'about', 'for', 'a', 'trace', 'of', 'such', 'activity', '.', '\"', 'Never', ',', '\"', 'he', 'said', 'briefly', '.', '\"', 'Or', 'water-colour', '--', 'or', 'etching', '?', '\"', 'His', 'confident', 'eyes', 'grew', 'dim', ',', 'and', 'his', 'cheeks', 'paled', 'a', 'little', 'under', 'their', 'handsome', 'sunburn', '.', '\"', 'Never', 'think', 'of', 'it', ',', 'my', 'dear', 'fellow', '--', 'any', 'more', 'than', 'if', 'I', \"'\", 'd', 'never', 'touched', 'a', 'brush', '.', '\"', 'And', 'his', 'tone', 'told', 'me', 'in', 'a', 'flash', 'that', 'he', 'never', 'thought', 'of', 'anything', 'else', '.', 'I', 'moved', 'away', ',', 'instinctively', 'embarrassed', 'by', 'my', 'unexpected', 'discovery', ';', 'and', 'as', 'I', 'turned', ',', 'my', 'eye', 'fell', 'on', 'a', 'small', 'picture', 'above', 'the', 'mantel-piece', '--', 'the', 'only', 'object', 'breaking', 'the', 'plain', 'oak', 'panelling', 'of', 'the', 'room', '.', '\"', 'Oh', ',', 'by', 'Jove', '!', '\"', 'I', 'said', '.', 'It', 'was', 'a', 'sketch', 'of', 'a', 'donkey', '--', 'an', 'old', 'tired', 'donkey', ',', 'standing', 'in', 'the', 'rain', 'under', 'a', 'wall', '.', '\"', 'By', 'Jove', '--', 'a', 'Stroud', '!', '\"', 'I', 'cried', '.', 'He', 'was', 'silent', ';', 'but', 'I', 'felt', 'him', 'close', 'behind', 'me', ',', 'breathing', 'a', 'little', 'quickly', '.', '\"', 'What', 'a', 'wonder', '!', 'Made', 'with', 'a', 'dozen', 'lines', '--', 'but', 'on', 'everlasting', 'foundations', '.', 'You', 'lucky', 'chap', ',', 'where', 'did', 'you', 'get', 'it', '?', '\"', 'He', 'answered', 'slowly', ':', '\"', 'Mrs', '.', 'Stroud', 'gave', 'it', 'to', 'me', '.', '\"', '\"', 'Ah', '--', 'I', 'didn', \"'\", 't', 'know', 'you', 'even', 'knew', 'the', 'Strouds', '.', 'He', 'was', 'such', 'an', 'inflexible', 'hermit', '.', '\"', '\"', 'I', 'didn', \"'\", 't', '--', 'till', 'after', '.', '.', '.', '.', 'She', 'sent', 'for', 'me', 'to', 'paint', 'him', 'when', 'he', 'was', 'dead', '.', '\"', '\"', 'When', 'he', 'was', 'dead', '?', 'You', '?', '\"', 'I', 'must', 'have', 'let', 'a', 'little', 'too', 'much', 'amazement', 'escape', 'through', 'my', 'surprise', ',', 'for', 'he', 'answered', 'with', 'a', 'deprecating', 'laugh', ':', '\"', 'Yes', '--', 'she', \"'\", 's', 'an', 'awful', 'simpleton', ',', 'you', 'know', ',', 'Mrs', '.', 'Stroud', '.', 'Her', 'only', 'idea', 'was', 'to', 'have', 'him', 'done', 'by', 'a', 'fashionable', 'painter', '--', 'ah', ',', 'poor', 'Stroud', '!', 'She', 'thought', 'it', 'the', 'surest', 'way', 'of', 'proclaiming', 'his', 'greatness', '--', 'of', 'forcing', 'it', 'on', 'a', 'purblind', 'public', '.', 'And', 'at', 'the', 'moment', 'I', 'was', '_', 'the', '_', 'fashionable', 'painter', '.', '\"', '\"', 'Ah', ',', 'poor', 'Stroud', '--', 'as', 'you', 'say', '.', 'Was', '_', 'that', '_', 'his', 'history', '?', '\"', '\"', 'That', 'was', 'his', 'history', '.', 'She', 'believed', 'in', 'him', ',', 'gloried', 'in', 'him', '--', 'or', 'thought', 'she', 'did', '.', 'But', 'she', 'couldn', \"'\", 't', 'bear', 'not', 'to', 'have', 'all', 'the', 'drawing-rooms', 'with', 'her', '.', 'She', 'couldn', \"'\", 't', 'bear', 'the', 'fact', 'that', ',', 'on', 'varnishing', 'days', ',', 'one', 'could', 'always', 'get', 'near', 'enough', 'to', 'see', 'his', 'pictures', '.', 'Poor', 'woman', '!', 'She', \"'\", 's', 'just', 'a', 'fragment', 'groping', 'for', 'other', 'fragments', '.', 'Stroud', 'is', 'the', 'only', 'whole', 'I', 'ever', 'knew', '.', '\"', '\"', 'You', 'ever', 'knew', '?', 'But', 'you', 'just', 'said', '--', '\"', 'Gisburn', 'had', 'a', 'curious', 'smile', 'in', 'his', 'eyes', '.', '\"', 'Oh', ',', 'I', 'knew', 'him', ',', 'and', 'he', 'knew', 'me', '--', 'only', 'it', 'happened', 'after', 'he', 'was', 'dead', '.', '\"', 'I', 'dropped', 'my', 'voice', 'instinctively', '.', '\"', 'When', 'she', 'sent', 'for', 'you', '?', '\"', '\"', 'Yes', '--', 'quite', 'insensible', 'to', 'the', 'irony', '.', 'She', 'wanted', 'him', 'vindicated', '--', 'and', 'by', 'me', '!', '\"', 'He', 'laughed', 'again', ',', 'and', 'threw', 'back', 'his', 'head', 'to', 'look', 'up', 'at', 'the', 'sketch', 'of', 'the', 'donkey', '.', '\"', 'There', 'were', 'days', 'when', 'I', 'couldn', \"'\", 't', 'look', 'at', 'that', 'thing', '--', 'couldn', \"'\", 't', 'face', 'it', '.', 'But', 'I', 'forced', 'myself', 'to', 'put', 'it', 'here', ';', 'and', 'now', 'it', \"'\", 's', 'cured', 'me', '--', 'cured', 'me', '.', 'That', \"'\", 's', 'the', 'reason', 'why', 'I', 'don', \"'\", 't', 'dabble', 'any', 'more', ',', 'my', 'dear', 'Rickham', ';', 'or', 'rather', 'Stroud', 'himself', 'is', 'the', 'reason', '.', '\"', 'For', 'the', 'first', 'time', 'my', 'idle', 'curiosity', 'about', 'my', 'companion', 'turned', 'into', 'a', 'serious', 'desire', 'to', 'understand', 'him', 'better', '.', '\"', 'I', 'wish', 'you', \"'\", 'd', 'tell', 'me', 'how', 'it', 'happened', ',', '\"', 'I', 'said', '.', 'He', 'stood', 'looking', 'up', 'at', 'the', 'sketch', ',', 'and', 'twirling', 'between', 'his', 'fingers', 'a', 'cigarette', 'he', 'had', 'forgotten', 'to', 'light', '.', 'Suddenly', 'he', 'turned', 'toward', 'me', '.', '\"', 'I', \"'\", 'd', 'rather', 'like', 'to', 'tell', 'you', '--', 'because', 'I', \"'\", 've', 'always', 'suspected', 'you', 'of', 'loathing', 'my', 'work', '.', '\"', 'I', 'made', 'a', 'deprecating', 'gesture', ',', 'which', 'he', 'negatived', 'with', 'a', 'good-humoured', 'shrug', '.', '\"', 'Oh', ',', 'I', 'didn', \"'\", 't', 'care', 'a', 'straw', 'when', 'I', 'believed', 'in', 'myself', '--', 'and', 'now', 'it', \"'\", 's', 'an', 'added', 'tie', 'between', 'us', '!', '\"', 'He', 'laughed', 'slightly', ',', 'without', 'bitterness', ',', 'and', 'pushed', 'one', 'of', 'the', 'deep', 'arm-chairs', 'forward', '.', '\"', 'There', ':', 'make', 'yourself', 'comfortable', '--', 'and', 'here', 'are', 'the', 'cigars', 'you', 'like', '.', '\"', 'He', 'placed', 'them', 'at', 'my', 'elbow', 'and', 'continued', 'to', 'wander', 'up', 'and', 'down', 'the', 'room', ',', 'stopping', 'now', 'and', 'then', 'beneath', 'the', 'picture', '.', '\"', 'How', 'it', 'happened', '?', 'I', 'can', 'tell', 'you', 'in', 'five', 'minutes', '--', 'and', 'it', 'didn', \"'\", 't', 'take', 'much', 'longer', 'to', 'happen', '.', '.', '.', '.', 'I', 'can', 'remember', 'now', 'how', 'surprised', 'and', 'pleased', 'I', 'was', 'when', 'I', 'got', 'Mrs', '.', 'Stroud', \"'\", 's', 'note', '.', 'Of', 'course', ',', 'deep', 'down', ',', 'I', 'had', 'always', '_', 'felt', '_', 'there', 'was', 'no', 'one', 'like', 'him', '--', 'only', 'I', 'had', 'gone', 'with', 'the', 'stream', ',', 'echoed', 'the', 'usual', 'platitudes', 'about', 'him', ',', 'till', 'I', 'half', 'got', 'to', 'think', 'he', 'was', 'a', 'failure', ',', 'one', 'of', 'the', 'kind', 'that', 'are', 'left', 'behind', '.', 'By', 'Jove', ',', 'and', 'he', '_', 'was', '_', 'left', 'behind', '--', 'because', 'he', 'had', 'come', 'to', 'stay', '!', 'The', 'rest', 'of', 'us', 'had', 'to', 'let', 'ourselves', 'be', 'swept', 'along', 'or', 'go', 'under', ',', 'but', 'he', 'was', 'high', 'above', 'the', 'current', '--', 'on', 'everlasting', 'foundations', ',', 'as', 'you', 'say', '.', '\"', 'Well', ',', 'I', 'went', 'off', 'to', 'the', 'house', 'in', 'my', 'most', 'egregious', 'mood', '--', 'rather', 'moved', ',', 'Lord', 'forgive', 'me', ',', 'at', 'the', 'pathos', 'of', 'poor', 'Stroud', \"'\", 's', 'career', 'of', 'failure', 'being', 'crowned', 'by', 'the', 'glory', 'of', 'my', 'painting', 'him', '!', 'Of', 'course', 'I', 'meant', 'to', 'do', 'the', 'picture', 'for', 'nothing', '--', 'I', 'told', 'Mrs', '.', 'Stroud', 'so', 'when', 'she', 'began', 'to', 'stammer', 'something', 'about', 'her', 'poverty', '.', 'I', 'remember', 'getting', 'off', 'a', 'prodigious', 'phrase', 'about', 'the', 'honour', 'being', '_', 'mine', '_', '--', 'oh', ',', 'I', 'was', 'princely', ',', 'my', 'dear', 'Rickham', '!', 'I', 'was', 'posing', 'to', 'myself', 'like', 'one', 'of', 'my', 'own', 'sitters', '.', '\"', 'Then', 'I', 'was', 'taken', 'up', 'and', 'left', 'alone', 'with', 'him', '.', 'I', 'had', 'sent', 'all', 'my', 'traps', 'in', 'advance', ',', 'and', 'I', 'had', 'only', 'to', 'set', 'up', 'the', 'easel', 'and', 'get', 'to', 'work', '.', 'He', 'had', 'been', 'dead', 'only', 'twenty-four', 'hours', ',', 'and', 'he', 'died', 'suddenly', ',', 'of', 'heart', 'disease', ',', 'so', 'that', 'there', 'had', 'been', 'no', 'preliminary', 'work', 'of', 'destruction', '--', 'his', 'face', 'was', 'clear', 'and', 'untouched', '.', 'I', 'had', 'met', 'him', 'once', 'or', 'twice', ',', 'years', 'before', ',', 'and', 'thought', 'him', 'insignificant', 'and', 'dingy', '.', 'Now', 'I', 'saw', 'that', 'he', 'was', 'superb', '.', '\"', 'I', 'was', 'glad', 'at', 'first', ',', 'with', 'a', 'merely', 'aesthetic', 'satisfaction', ':', 'glad', 'to', 'have', 'my', 'hand', 'on', 'such', 'a', \"'\", 'subject', '.', \"'\", 'Then', 'his', 'strange', 'life-likeness', 'began', 'to', 'affect', 'me', 'queerly', '--', 'as', 'I', 'blocked', 'the', 'head', 'in', 'I', 'felt', 'as', 'if', 'he', 'were', 'watching', 'me', 'do', 'it', '.', 'The', 'sensation', 'was', 'followed', 'by', 'the', 'thought', ':', 'if', 'he', '_', 'were', '_', 'watching', 'me', ',', 'what', 'would', 'he', 'say', 'to', 'my', 'way', 'of', 'working', '?', 'My', 'strokes', 'began', 'to', 'go', 'a', 'little', 'wild', '--', 'I', 'felt', 'nervous', 'and', 'uncertain', '.', '\"', 'Once', ',', 'when', 'I', 'looked', 'up', ',', 'I', 'seemed', 'to', 'see', 'a', 'smile', 'behind', 'his', 'close', 'grayish', 'beard', '--', 'as', 'if', 'he', 'had', 'the', 'secret', ',', 'and', 'were', 'amusing', 'himself', 'by', 'holding', 'it', 'back', 'from', 'me', '.', 'That', 'exasperated', 'me', 'still', 'more', '.', 'The', 'secret', '?', 'Why', ',', 'I', 'had', 'a', 'secret', 'worth', 'twenty', 'of', 'his', '!', 'I', 'dashed', 'at', 'the', 'canvas', 'furiously', ',', 'and', 'tried', 'some', 'of', 'my', 'bravura', 'tricks', '.', 'But', 'they', 'failed', 'me', ',', 'they', 'crumbled', '.', 'I', 'saw', 'that', 'he', 'wasn', \"'\", 't', 'watching', 'the', 'showy', 'bits', '--', 'I', 'couldn', \"'\", 't', 'distract', 'his', 'attention', ';', 'he', 'just', 'kept', 'his', 'eyes', 'on', 'the', 'hard', 'passages', 'between', '.', 'Those', 'were', 'the', 'ones', 'I', 'had', 'always', 'shirked', ',', 'or', 'covered', 'up', 'with', 'some', 'lying', 'paint', '.', 'And', 'how', 'he', 'saw', 'through', 'my', 'lies', '!', '\"', 'I', 'looked', 'up', 'again', ',', 'and', 'caught', 'sight', 'of', 'that', 'sketch', 'of', 'the', 'donkey', 'hanging', 'on', 'the', 'wall', 'near', 'his', 'bed', '.', 'His', 'wife', 'told', 'me', 'afterward', 'it', 'was', 'the', 'last', 'thing', 'he', 'had', 'done', '--', 'just', 'a', 'note', 'taken', 'with', 'a', 'shaking', 'hand', ',', 'when', 'he', 'was', 'down', 'in', 'Devonshire', 'recovering', 'from', 'a', 'previous', 'heart', 'attack', '.', 'Just', 'a', 'note', '!', 'But', 'it', 'tells', 'his', 'whole', 'history', '.', 'There', 'are', 'years', 'of', 'patient', 'scornful', 'persistence', 'in', 'every', 'line', '.', 'A', 'man', 'who', 'had', 'swum', 'with', 'the', 'current', 'could', 'never', 'have', 'learned', 'that', 'mighty', 'up-stream', 'stroke', '.', '.', '.', '.', '\"', 'I', 'turned', 'back', 'to', 'my', 'work', ',', 'and', 'went', 'on', 'groping', 'and', 'muddling', ';', 'then', 'I', 'looked', 'at', 'the', 'donkey', 'again', '.', 'I', 'saw', 'that', ',', 'when', 'Stroud', 'laid', 'in', 'the', 'first', 'stroke', ',', 'he', 'knew', 'just', 'what', 'the', 'end', 'would', 'be', '.', 'He', 'had', 'possessed', 'his', 'subject', ',', 'absorbed', 'it', ',', 'recreated', 'it', '.', 'When', 'had', 'I', 'done', 'that', 'with', 'any', 'of', 'my', 'things', '?', 'They', 'hadn', \"'\", 't', 'been', 'born', 'of', 'me', '--', 'I', 'had', 'just', 'adopted', 'them', '.', '.', '.', '.', '\"', 'Hang', 'it', ',', 'Rickham', ',', 'with', 'that', 'face', 'watching', 'me', 'I', 'couldn', \"'\", 't', 'do', 'another', 'stroke', '.', 'The', 'plain', 'truth', 'was', ',', 'I', 'didn', \"'\", 't', 'know', 'where', 'to', 'put', 'it', '--', '_', 'I', 'had', 'never', 'known', '_', '.', 'Only', ',', 'with', 'my', 'sitters', 'and', 'my', 'public', ',', 'a', 'showy', 'splash', 'of', 'colour', 'covered', 'up', 'the', 'fact', '--', 'I', 'just', 'threw', 'paint', 'into', 'their', 'faces', '.', '.', '.', '.', 'Well', ',', 'paint', 'was', 'the', 'one', 'medium', 'those', 'dead', 'eyes', 'could', 'see', 'through', '--', 'see', 'straight', 'to', 'the', 'tottering', 'foundations', 'underneath', '.', 'Don', \"'\", 't', 'you', 'know', 'how', ',', 'in', 'talking', 'a', 'foreign', 'language', ',', 'even', 'fluently', ',', 'one', 'says', 'half', 'the', 'time', 'not', 'what', 'one', 'wants', 'to', 'but', 'what', 'one', 'can', '?', 'Well', '--', 'that', 'was', 'the', 'way', 'I', 'painted', ';', 'and', 'as', 'he', 'lay', 'there', 'and', 'watched', 'me', ',', 'the', 'thing', 'they', 'called', 'my', \"'\", 'technique', \"'\", 'collapsed', 'like', 'a', 'house', 'of', 'cards', '.', 'He', 'didn', \"'\", 't', 'sneer', ',', 'you', 'understand', ',', 'poor', 'Stroud', '--', 'he', 'just', 'lay', 'there', 'quietly', 'watching', ',', 'and', 'on', 'his', 'lips', ',', 'through', 'the', 'gray', 'beard', ',', 'I', 'seemed', 'to', 'hear', 'the', 'question', ':', \"'\", 'Are', 'you', 'sure', 'you', 'know', 'where', 'you', \"'\", 're', 'coming', 'out', '?', \"'\", '\"', 'If', 'I', 'could', 'have', 'painted', 'that', 'face', ',', 'with', 'that', 'question', 'on', 'it', ',', 'I', 'should', 'have', 'done', 'a', 'great', 'thing', '.', 'The', 'next', 'greatest', 'thing', 'was', 'to', 'see', 'that', 'I', 'couldn', \"'\", 't', '--', 'and', 'that', 'grace', 'was', 'given', 'me', '.', 'But', ',', 'oh', ',', 'at', 'that', 'minute', ',', 'Rickham', ',', 'was', 'there', 'anything', 'on', 'earth', 'I', 'wouldn', \"'\", 't', 'have', 'given', 'to', 'have', 'Stroud', 'alive', 'before', 'me', ',', 'and', 'to', 'hear', 'him', 'say', ':', \"'\", 'It', \"'\", 's', 'not', 'too', 'late', '--', 'I', \"'\", 'll', 'show', 'you', 'how', \"'\", '?', '\"', 'It', '_', 'was', '_', 'too', 'late', '--', 'it', 'would', 'have', 'been', ',', 'even', 'if', 'he', \"'\", 'd', 'been', 'alive', '.', 'I', 'packed', 'up', 'my', 'traps', ',', 'and', 'went', 'down', 'and', 'told', 'Mrs', '.', 'Stroud', '.', 'Of', 'course', 'I', 'didn', \"'\", 't', 'tell', 'her', '_', 'that', '_', '--', 'it', 'would', 'have', 'been', 'Greek', 'to', 'her', '.', 'I', 'simply', 'said', 'I', 'couldn', \"'\", 't', 'paint', 'him', ',', 'that', 'I', 'was', 'too', 'moved', '.', 'She', 'rather', 'liked', 'the', 'idea', '--', 'she', \"'\", 's', 'so', 'romantic', '!', 'It', 'was', 'that', 'that', 'made', 'her', 'give', 'me', 'the', 'donkey', '.', 'But', 'she', 'was', 'terribly', 'upset', 'at', 'not', 'getting', 'the', 'portrait', '--', 'she', 'did', 'so', 'want', 'him', \"'\", 'done', \"'\", 'by', 'some', 'one', 'showy', '!', 'At', 'first', 'I', 'was', 'afraid', 'she', 'wouldn', \"'\", 't', 'let', 'me', 'off', '--', 'and', 'at', 'my', 'wits', \"'\", 'end', 'I', 'suggested', 'Grindle', '.', 'Yes', ',', 'it', 'was', 'I', 'who', 'started', 'Grindle', ':', 'I', 'told', 'Mrs', '.', 'Stroud', 'he', 'was', 'the', \"'\", 'coming', \"'\", 'man', ',', 'and', 'she', 'told', 'somebody', 'else', ',', 'and', 'so', 'it', 'got', 'to', 'be', 'true', '.', '.', '.', '.', 'And', 'he', 'painted', 'Stroud', 'without', 'wincing', ';', 'and', 'she', 'hung', 'the', 'picture', 'among', 'her', 'husband', \"'\", 's', 'things', '.', '.', '.', '.', '\"', 'He', 'flung', 'himself', 'down', 'in', 'the', 'arm-chair', 'near', 'mine', ',', 'laid', 'back', 'his', 'head', ',', 'and', 'clasping', 'his', 'arms', 'beneath', 'it', ',', 'looked', 'up', 'at', 'the', 'picture', 'above', 'the', 'chimney-piece', '.', '\"', 'I', 'like', 'to', 'fancy', 'that', 'Stroud', 'himself', 'would', 'have', 'given', 'it', 'to', 'me', ',', 'if', 'he', \"'\", 'd', 'been', 'able', 'to', 'say', 'what', 'he', 'thought', 'that', 'day', '.', '\"', 'And', ',', 'in', 'answer', 'to', 'a', 'question', 'I', 'put', 'half-mechanically', '--', '\"', 'Begin', 'again', '?', '\"', 'he', 'flashed', 'out', '.', '\"', 'When', 'the', 'one', 'thing', 'that', 'brings', 'me', 'anywhere', 'near', 'him', 'is', 'that', 'I', 'knew', 'enough', 'to', 'leave', 'off', '?', '\"', 'He', 'stood', 'up', 'and', 'laid', 'his', 'hand', 'on', 'my', 'shoulder', 'with', 'a', 'laugh', '.', '\"', 'Only', 'the', 'irony', 'of', 'it', 'is', 'that', 'I', '_', 'am', '_', 'still', 'painting', '--', 'since', 'Grindle', \"'\", 's', 'doing', 'it', 'for', 'me', '!', 'The', 'Strouds', 'stand', 'alone', ',', 'and', 'happen', 'once', '--', 'but', 'there', \"'\", 's', 'no', 'exterminating', 'our', 'kind', 'of', 'art', '.', '\"']\n",
      "\n",
      "üìò Vocabulary Created:\n",
      "Total Tokens: 1132\n",
      "[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)] ...\n",
      "\n",
      "üî¢ Encoded IDs:\n",
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\n",
      "üó£Ô∏è Decoded Text:\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
      "\n",
      "üìú Joined Text with <|endoftext|> token:\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "\n",
      "üî¢ Encoded IDs for Joined Text:\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "\n",
      "üó£Ô∏è Decoded Joined Text:\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "from tiny_gpt import RegexTokenizer\n",
    "\n",
    "\n",
    "\n",
    "rt_obj = RegexTokenizer(raw_text_data)\n",
    "\n",
    "# Step 1: Split the text into tokens\n",
    "vocab_list = rt_obj.split()\n",
    "print(\"\\nüß© Tokenized Vocabulary List:\")\n",
    "print(vocab_list)\n",
    "\n",
    "# Step 2: Create the vocabulary mappings (token <-> ID)\n",
    "vocab = rt_obj.create_vocabulary()\n",
    "print(\"\\nüìò Vocabulary Created:\")\n",
    "print(f\"Total Tokens: {len(vocab)}\")\n",
    "print(list(vocab.items())[:10], \"...\")  # print first 10 tokens\n",
    "\n",
    "# Step 3: Encode a sample text into IDs\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = rt_obj.encode(text)\n",
    "print(\"\\nüî¢ Encoded IDs:\")\n",
    "print(ids)\n",
    "\n",
    "# Step 4: Decode the IDs back into readable text\n",
    "decoded_text = rt_obj.decode(ids)\n",
    "print(\"\\nüó£Ô∏è Decoded Text:\")\n",
    "print(decoded_text)\n",
    "\n",
    "# Step 5: Encode text with <|endoftext|> separators (like GPT-style)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "joined_text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(\"\\nüìú Joined Text with <|endoftext|> token:\")\n",
    "print(joined_text)\n",
    "\n",
    "# Step 6: Encode and decode the joined text\n",
    "ids = rt_obj.encode(joined_text)\n",
    "print(\"\\nüî¢ Encoded IDs for Joined Text:\")\n",
    "print(ids)\n",
    "\n",
    "decoded_joined_text = rt_obj.decode(ids)\n",
    "print(\"\\nüó£Ô∏è Decoded Joined Text:\")\n",
    "print(decoded_joined_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42abd38",
   "metadata": {},
   "source": [
    "- [BOS] (beginning of sequence)‚Äâ‚ÄîThis token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
    "- [EOS] (end of sequence)‚Äâ‚ÄîThis token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins.\n",
    "- [PAD] (padding)‚Äâ‚ÄîWhen training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or ‚Äúpadded‚Äù using the [PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d1c62",
   "metadata": {},
   "source": [
    "### TikToken Byte Pair Encoding\n",
    "\n",
    "- In this section, we perform tokenization using **Byte Pair Encoding (BPE)** provided by the `tiktoken` library.\n",
    "- We use the **GPT-2 tokenizer** as our base model for encoding the text.\n",
    "\n",
    "- The algorithm underlying BPE breaks down words that aren‚Äôt in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words. So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b51e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Joined Text with <|endoftext|> token:\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "\n",
      "Encoded Token IDs:\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n",
      "\n",
      "Decoded Text:\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "\n",
      "Total Tokens: 19\n"
     ]
    }
   ],
   "source": [
    "# Example: Encoding and decoding text using TikTokenizer\n",
    "\n",
    "from tiny_gpt import TikTokenizer\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "joined_text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\"\\nJoined Text with <|endoftext|> token:\")\n",
    "print(joined_text)\n",
    "\n",
    "\n",
    "# Initialize tokenizer\n",
    "tik_obj = TikTokenizer(\"gpt2\", {\"<|endoftext|>\"})\n",
    "\n",
    "# Encode text to token IDs\n",
    "encoded_ids = tik_obj.encode(joined_text)\n",
    "print(\"\\nEncoded Token IDs:\")\n",
    "print(encoded_ids)\n",
    "\n",
    "# Decode token IDs back to text\n",
    "decoded_text = tik_obj.decode(encoded_ids)\n",
    "print(\"\\nDecoded Text:\")\n",
    "print(decoded_text)\n",
    "\n",
    "# Optional: Show token count\n",
    "print(\"\\nTotal Tokens:\", tik_obj.count_tokens(joined_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586be6e",
   "metadata": {},
   "source": [
    "## Part B - Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52518ff7",
   "metadata": {},
   "source": [
    "# LLM Training Notes\n",
    "\n",
    "1. Tokenization\n",
    "\n",
    "So far, we‚Äôve explored how to convert raw text into tokens, using methods like:\n",
    "\n",
    "- Regex tokenization ‚Äì simple rule-based splitting of text.\n",
    "\n",
    "- BPE (Byte Pair Encoding) ‚Äì merges frequent character pairs into larger units, balancing efficiency and vocabulary size.\n",
    "\n",
    "- Tokens are the smallest meaningful units the model works with.\n",
    "\n",
    "2. Dataset and Dataloader\n",
    "\n",
    "To train a model, we need to feed data in a structured way ‚Äî that‚Äôs where the dataset and dataloader come in.\n",
    "\n",
    "- Dataset\n",
    "\n",
    "  - The dataset is a subclass of torch.utils.data.Dataset.\n",
    "\n",
    "  It contains:\n",
    "\n",
    "  - Input tokens (the source text as token IDs)\n",
    "\n",
    "  - Targets (the shifted version of input tokens)\n",
    "\n",
    "- Inputs are created using:\n",
    "\n",
    "  - context_length ‚Üí defines how many tokens the model can see at once.\n",
    "\n",
    "  - stride ‚Üí defines how much to move (or slide) the input window forward for the next sample.\n",
    "\n",
    "- For example:\n",
    "\n",
    "  tokens: [A, B, C, D, E, F]\n",
    "  context_length = 4\n",
    "  stride = 2\n",
    "\n",
    "  ‚Üí samples:\n",
    "\n",
    "  Input 1: [A, B, C, D]\n",
    "\n",
    "  Input 2: [C, D, E, F]\n",
    "\n",
    "Each input has a corresponding target, usually the same sequence shifted by one token.\n",
    "LLMs are trained to predict the next token given the previous ones.\n",
    "\n",
    "- Dataloader\n",
    "\n",
    "The dataloader takes the dataset and:\n",
    "\n",
    "- Divides it into batches (controlled by batch_size)\n",
    "\n",
    "- Optionally shuffles the samples\n",
    "\n",
    "- Can drop the last incomplete batch if needed\n",
    "\n",
    "Each batch contains multiple input sequences (and their targets) that are fed together into the model during training.\n",
    "\n",
    "This batching helps with:\n",
    "\n",
    "- Parallelization on GPUs\n",
    "\n",
    "- Stabilizing gradients through averaging across samples\n",
    "\n",
    "3. Token Embeddings\n",
    "\n",
    "The inputs from the dataloader are still integer token IDs.\n",
    "Before feeding them into the transformer, they must be converted into dense, learnable vector representations called token embeddings.\n",
    "\n",
    "The embedding layer maps each token ID ‚Üí a high-dimensional vector.\n",
    "\n",
    "Each dimension represents a different learned ‚Äúfeature‚Äù or aspect of meaning.\n",
    "\n",
    "These embeddings form the model‚Äôs input representation space.\n",
    "\n",
    "4. Positional Embeddings\n",
    "\n",
    "Transformers have no inherent sense of order ‚Äî self-attention treats all tokens equally.\n",
    "To fix this, we add positional embeddings to the token embeddings.\n",
    "\n",
    "They encode where each token appears in the sequence.\n",
    "\n",
    "When added to token embeddings, they allow the model to understand the relative position of words/tokens (e.g., ‚Äúcat sat on mat‚Äù ‚â† ‚Äúmat sat on cat‚Äù).\n",
    "\n",
    "```\n",
    "Raw text\n",
    "   ‚Üì\n",
    "Tokenizer ‚Üí Tokens\n",
    "   ‚Üì\n",
    "Dataset (creates input-target pairs using stride & context length)\n",
    "   ‚Üì\n",
    "Dataloader (batches & shuffles data)\n",
    "   ‚Üì\n",
    "Token Embeddings (map token IDs ‚Üí dense vectors)\n",
    "   ‚Üì\n",
    "+ Positional Embeddings (add sequence order info)\n",
    "   ‚Üì\n",
    "Transformer (self-attention, training, etc.)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c61ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride = 1\n",
      "First Batch\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Second Batch\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
      "--------------------------------------------------\n",
      "Stride = 2\n",
      "First Batch\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Second Batch\n",
      "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n",
      "--------------------------------------------------\n",
      "Stride = 4\n",
      "First Batch\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Second Batch\n",
      "[tensor([[1807, 3619,  402,  271]]), tensor([[ 3619,   402,   271, 10899]])]\n",
      "--------------------------------------------------\n",
      "Batch size 4 ,Stride = 2\n",
      "First Batch\n",
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [  402,   271, 10899,  2138]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  271, 10899,  2138,   257]])]\n",
      "Second Batch\n",
      "[tensor([[10899,  2138,   257,  7026],\n",
      "        [  257,  7026, 15632,   438],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [ 2016,   257,   922,  5891]]), tensor([[ 2138,   257,  7026, 15632],\n",
      "        [ 7026, 15632,   438,  2016],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [  257,   922,  5891,  1576]])]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tiny_gpt import tiny_data_loader\n",
    "\n",
    "# ----------------------------- #\n",
    "# Example: Understanding stride and batch behavior\n",
    "# ----------------------------- #\n",
    "\n",
    "print(\"Stride = 1\")\n",
    "# Create dataloader with stride=1 ‚Üí input window slides by 1 token each time\n",
    "data_loader = tiny_data_loader(\n",
    "    raw_text_data, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# Each batch contains (input_ids, target_ids)\n",
    "first_batch = next(data_iter)\n",
    "print(\"First Batch\")\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(\"Second Batch\")\n",
    "print(second_batch)\n",
    "\n",
    "print(50 * \"-\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Stride = 2 ‚Üí input moves 2 tokens ahead between samples\n",
    "# Target is still shifted by 1 internally (always next-token prediction)\n",
    "# ----------------------------- #\n",
    "print(\"Stride = 2\")\n",
    "data_loader = tiny_data_loader(\n",
    "    raw_text_data, batch_size=1, max_length=4, stride=2, shuffle=False\n",
    ")\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(\"First Batch\")\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(\"Second Batch\")\n",
    "print(second_batch)\n",
    "\n",
    "print(50 * \"-\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Stride = 4 ‚Üí no overlap, input window jumps completely to the next chunk\n",
    "# ----------------------------- #\n",
    "print(\"Stride = 4\")\n",
    "data_loader = tiny_data_loader(\n",
    "    raw_text_data, batch_size=1, max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(\"First Batch\")\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(\"Second Batch\")\n",
    "print(second_batch)\n",
    "\n",
    "print(50 * \"-\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Batch size = 4, Stride = 2\n",
    "# Multiple samples are grouped together in one batch\n",
    "# Each element in the batch is an (input, target) pair\n",
    "# ----------------------------- #\n",
    "print(\"Batch size 4 ,Stride = 2\")\n",
    "data_loader = tiny_data_loader(\n",
    "    raw_text_data, batch_size=4, max_length=4, stride=2, shuffle=False\n",
    ")\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(\"First Batch\")\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(\"Second Batch\")\n",
    "print(second_batch)\n",
    "\n",
    "print(50 * \"-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75450210",
   "metadata": {},
   "source": [
    "## Create Input Embeddings (Token Embeddings + Position Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ead3b71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Total tokens present in the vocabulary\n",
      "50257\n",
      "Embedding(50257, 256)\n",
      "--------------------------------------------------\n",
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  257,   922,  5891,  1576],\n",
      "        [ 1576,   438,   568,   340],\n",
      "        [  340,   373,   645,  1049]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "--------------------------------------------------\n",
      "Token embeddings shape: torch.Size([8, 4, 256])\n",
      "--------------------------------------------------\n",
      "Positional embeddings shape: torch.Size([4, 256])\n",
      "Final input embeddings shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tiny_gpt import tiny_data_loader\n",
    "from tiny_gpt import TikTokenizer  # assuming your tokenizer class is here\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialize tokenizer and get vocabulary info\n",
    "# ------------------------------------------------------------\n",
    "tik_obj = TikTokenizer(\"gpt2\", {\"<|endoftext|>\"})\n",
    "\n",
    "print(50 * \"-\")\n",
    "print(\"Total tokens present in the vocabulary\")\n",
    "print(tik_obj.get_n_vocab_tokens())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create token embedding layer\n",
    "# ------------------------------------------------------------\n",
    "vocab_size = tik_obj.get_n_vocab_tokens()  # total tokens in tokenizer vocabulary\n",
    "output_dim = 256  # embedding vector dimension (hidden size)\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(token_embedding_layer)\n",
    "\n",
    "print(50 * \"-\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Prepare a simple dataloader\n",
    "# ------------------------------------------------------------\n",
    "max_length = 4\n",
    "dataloader = tiny_data_loader(\n",
    "    raw_text_data,\n",
    "    batch_size=8,\n",
    "    max_length=max_length,\n",
    "    stride=max_length - 1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Convert token IDs to embeddings\n",
    "# ------------------------------------------------------------\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Token embeddings shape:\", token_embeddings.shape)\n",
    "print(50 * \"-\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create position embeddings\n",
    "# ------------------------------------------------------------\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# Get positional embeddings for each position in the input (0, 1, 2, 3)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Positional embeddings shape:\", pos_embeddings.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Combine token + positional embeddings\n",
    "# ------------------------------------------------------------\n",
    "# Broadcasting automatically matches positions for each token in the batch\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Final input embeddings shape:\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0a442",
   "metadata": {},
   "source": [
    "## Token and Positional Embeddings ‚Äî Shape, Broadcasting, and Learning Behavior\n",
    "\n",
    "1. Token Embeddings\n",
    "\n",
    "Defined as:\n",
    "\n",
    "```\n",
    "    token_embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "    token_embeddings = token_embedding_layer(input_ids)\n",
    "\n",
    "```\n",
    "\n",
    "Shape of input_ids: (batch_size, seq_len)\n",
    "\n",
    "Shape of token_embeddings: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "Each token ID is mapped to a vector of size embed_dim.\n",
    "This represents the semantic meaning of that token (like a feature vector for each word).\n",
    "\n",
    "2. Positional Embeddings\n",
    "\n",
    "Defined as:\n",
    "\n",
    "```\n",
    "    pos_embedding_layer = torch.nn.Embedding(context_length, embed_dim)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(seq_len))\n",
    "```\n",
    "\n",
    "Shape of pos_embeddings: (seq_len, embed_dim)\n",
    "\n",
    "This layer assigns a unique embedding vector to each position in the input sequence (0, 1, 2, ‚Ä¶).\n",
    "Initially, these vectors are randomly initialized but learnable.\n",
    "\n",
    "3. Addition via Broadcasting\n",
    "\n",
    "When we add them:\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "PyTorch applies broadcasting automatically because:\n",
    "\n",
    "token_embeddings has shape (batch_size, seq_len, embed_dim)\n",
    "\n",
    "pos_embeddings has shape (seq_len, embed_dim)\n",
    "\n",
    "Broadcasting expands pos_embeddings along the batch dimension:\n",
    "\n",
    "```\n",
    "(batch_size, seq_len, embed_dim)\n",
    "\n",
    "-           (seq_len, embed_dim)\n",
    "\n",
    "---\n",
    "\n",
    "= (batch_size, seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "Effectively, each sequence in the batch gets the same positional offsets added to its token embeddings.\n",
    "\n",
    "So the i-th token in every sequence gets the same position vector added ‚Äî e.g.:\n",
    "\n",
    "input_embeddings[b, i, :] = token_embeddings[b, i, :] + pos_embeddings[i, :]\n",
    "\n",
    "4. Why Addition Works\n",
    "\n",
    "Adding these two embeddings lets the model encode both meaning and order:\n",
    "\n",
    "token_embeddings ‚Üí represent what the token is\n",
    "\n",
    "pos_embeddings ‚Üí represent where it appears in the sequence\n",
    "\n",
    "After addition, each token‚Äôs final embedding uniquely represents both its identity and its position.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "    token(\"cat\") + pos(0) ‚Üí \"cat at start\"\n",
    "    token(\"cat\") + pos(3) ‚Üí \"cat at end\"\n",
    "```\n",
    "\n",
    "\n",
    "Without this, ‚Äúcat‚Äù at position 0 and ‚Äúcat‚Äù at position 3 would look identical to the model.\n",
    "\n",
    "5. How Learning Differentiates Between Them\n",
    "\n",
    "Both embedding layers (token_embedding_layer and pos_embedding_layer) have independent learnable weights.\n",
    "During backpropagation:\n",
    "\n",
    "Gradients flow separately into each layer.\n",
    "\n",
    "token_embedding_layer.weight learns semantic relationships between tokens.\n",
    "\n",
    "pos_embedding_layer.weight learns how position affects meaning.\n",
    "\n",
    "Over many training steps:\n",
    "\n",
    "The token embeddings form clusters for semantically similar words (e.g., ‚Äúdog‚Äù and ‚Äúcat‚Äù).\n",
    "\n",
    "The position embeddings start encoding relative position information, such as:\n",
    "\n",
    "Early tokens ‚Üí may influence beginnings of sentences.\n",
    "\n",
    "Later tokens ‚Üí may correlate with sentence endings.\n",
    "\n",
    "Even though position embeddings start as random, they converge into structured, meaningful representations that help the model attend correctly to token order.\n",
    "\n",
    "‚úÖ In short:\n",
    "\n",
    "| Concept                  | Purpose                   | Shape                              | Learns? | Notes                       |\n",
    "| ------------------------ | ------------------------- | ---------------------------------- | ------- | --------------------------- |\n",
    "| **Token Embedding**      | Represents token meaning  | `(batch_size, seq_len, embed_dim)` | ‚úÖ Yes  | Learns vocabulary semantics |\n",
    "| **Positional Embedding** | Represents token position | `(seq_len, embed_dim)`             | ‚úÖ Yes  | Learns order patterns       |\n",
    "| **Combined Embedding**   | Input to Transformer      | `(batch_size, seq_len, embed_dim)` | ‚úÖ Yes  | Added via broadcasting      |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinygpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
