{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook is to implement a basic LLM fully from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization in Early LLM Stages\n",
    "\n",
    "In the initial stages of large language models (LLMs), the process begins with tokenizing both words and characters. Tokenization refers to the technique of converting text into numerical representations. There are various methods to achieve this:\n",
    "\n",
    "- **Byte Pair Encoding (BPE):** This method, used by OpenAI, breaks words down into subword units. For example, words like \"depend\" might be split into \"de\" and \"pend,\" while suffixes such as \"ing\" are treated similarly.\n",
    "  \n",
    "- **Word-to-Vector Encoding:** This simpler approach converts entire words into numerical values by sorting a dictionary of words and mapping each to a number. While straightforward, it struggles to handle unseen words.\n",
    "  \n",
    "In contrast, BPE is preferred because it breaks words into smaller subword units, ensuring even previously unseen words can be tokenized effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n",
      "torch version:  2.4.1+cu124\n"
     ]
    }
   ],
   "source": [
    "# Tiktoken is used for BPE\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", tiktoken.__version__)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(\"torch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the BPE\n",
    "- Chosen cl100k_base it has total 100256 tokens placed in the url of open ai \"https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 11, 656, 499, 1093, 15600, 30, 220, 100257, 763, 279, 7160, 32735, 7317, 2492, 1073, 1063, 16476, 17826, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "decode_text = tokenizer.decode(integers)\n",
    "print(decode_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the Data\n",
    "We will now define the input data and target data for our model:\n",
    "\n",
    "- **Input Data:** These are the tokens provided as input to the language model (LLM).\n",
    "- **Target Data:** This is the same as the input data, but used for prediction â€” it represents the next word that the model is trying to predict.\n",
    "- **Dataset:** The dataset in pytorch is an interface for accessing and managing data. len() and getitem() functions are needed for the dataset\n",
    "- **DataLoader:** : This used to load the data from the dataset in batches. It handles shuffling, batches and parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)    #1\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):     #2\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):    #3\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):         #4\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")                         #1\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)   #2\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,     #3\n",
    "        num_workers=num_workers     #4\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])]\n",
      "[tensor([[  287,   262,  6001,   286],\n",
      "        [  465, 13476,    11,   339],\n",
      "        [  550,  5710,   465, 12036],\n",
      "        [   11,  6405,   257,  5527],\n",
      "        [27075,    11,   290,  4920],\n",
      "        [ 2241,   287,   257,  4489],\n",
      "        [   64,   319,   262, 34686],\n",
      "        [41976,    13,   357, 10915]]), tensor([[  262,  6001,   286,   465],\n",
      "        [13476,    11,   339,   550],\n",
      "        [ 5710,   465, 12036,    11],\n",
      "        [ 6405,   257,  5527, 27075],\n",
      "        [   11,   290,  4920,  2241],\n",
      "        [  287,   257,  4489,    64],\n",
      "        [  319,   262, 34686, 41976],\n",
      "        [   13,   357, 10915,   314]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"The_Verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)      #1\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to create token embeddings\n",
    "These are like weights for data features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
